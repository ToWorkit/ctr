{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from process.ipynb\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Ipynb_importer\n",
    "# 自定义预处理\n",
    "import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TFIDF():\n",
    "#     \"\"\"利用单词的tfidf和consine计算query与doc的相关性\"\"\"\n",
    "#     def __init__(self, stopWordsPath, fileTitle, fileIntro):\n",
    "#         initData=process.Init()\n",
    "#         # 停用词\n",
    "#         self.stopWords=initData.loadStopWords(stopWordsPath)\n",
    "#         # 数据集 title，word\n",
    "#         self.filmTitles,self.filmDocs=initData.readData(fileTitle,fileIntro)\n",
    "        \n",
    "#     def cutText(self):\n",
    "#         \"\"\"对文章进行切词\"\"\"\n",
    "#         cutFileDocs = []\n",
    "#         for filmDoc in self.filmDocs:\n",
    "#             wordsList=self.segment(filmDoc)\n",
    "#             cutFileDocs.append(wordsList)\n",
    "#         return cutFileDocs\n",
    "        \n",
    "#     def segment(self, text):\n",
    "#         \"\"\"分词\"\"\"\n",
    "#         words = jieba.cut(text)\n",
    "#         # 停用词过滤\n",
    "#         wordsList = [word for word in words if word in self.stopWords]\n",
    "#         return \" \".join(wordsList)\n",
    "    \n",
    "#     def tfIDFSearch(self, query):\n",
    "#         cutFileDocs = self.cutText()\n",
    "#         # 文本转为TF-IDF特征矩阵\n",
    "#         tfIDf = TfidfVectorizer(min_df=1)  # min_df=1 过滤低于给出阈值的文档频率的词\n",
    "#         tfIDF_fit = tfIDf.fit(cutFileDocs)  # 开始训练\n",
    "#         tfIDF_vec = tfIDF_fit.transform(cutFileDocs)  # 转换文档为 文档-词矩阵\n",
    "        \n",
    "#         # 需要将查询语句转转为词向量\n",
    "#         queryWords =[self.segment(query)]\n",
    "#         # 转换文档为 文档-词矩阵\n",
    "#         queryVec=tfIDF_fit.transform(queryWords)\n",
    "#         # 余弦相似度\n",
    "#         similarity=cosine_similarity(queryVec,tfIDF_vec)[0]\n",
    "        \n",
    "#         similarity_index=similarity.argsort()[::-1]# 最相关的排在最前，返回index\n",
    "        \n",
    "        \n",
    "#         #返回前5个最相关的电影\n",
    "#         for index in list(similarity_index)[:5]:\n",
    "#             print('sim: %f,title: %s' %(similarity[index],self.filmTitles[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "利用单词的tfidf和consine计算query与doc的相关性\n",
    "'''\n",
    "class TFIDF():\n",
    "    def __init__(self, stopWordsPath, fileTitle, fileIntro):\n",
    "        initData=process.Init()\n",
    "        self.stopWords=initData.loadStopWords(stopWordsPath)\n",
    "        self.filmTitles,self.filmDocs=initData.readData(fileTitle,fileIntro)\n",
    "\n",
    "    def cutText(self):\n",
    "        cutFileDocs = []\n",
    "        for filmDoc in self.filmDocs:\n",
    "            wordsList=self.segment(filmDoc)\n",
    "            cutFileDocs.append(wordsList)\n",
    "        return cutFileDocs\n",
    "\n",
    "    def segment(self,text):\n",
    "        words=jieba.cut(text)#jieba分词\n",
    "        wordsList=[word for word in words if word not in self.stopWords]\n",
    "        return ' '.join(wordsList)\n",
    "\n",
    "    def tfIDFSearch(self, query):\n",
    "        cutFileDocs = self.cutText()\n",
    "        # 文本转为TF-IDF特征矩阵\n",
    "        tfIDf = TfidfVectorizer(min_df=1)  # min_df=1 过滤低于给出阈值的文档频率的词\n",
    "        tfIDF_fit = tfIDf.fit(cutFileDocs)  # 开始训练\n",
    "        tfIDF_vec = tfIDF_fit.transform(cutFileDocs)  # 转换文档为文档-词矩阵\n",
    "\n",
    "        # 需要将查询语句转转为词向量\n",
    "        queryWords =[self.segment(query)]\n",
    "        queryVec=tfIDF_fit.transform(queryWords)\n",
    "        similarity=cosine_similarity(queryVec,tfIDF_vec)[0]\n",
    "        print(similarity)\n",
    "        similarity_index=similarity.argsort()[::-1]#最相关的排在最前，返回index\n",
    "        \n",
    "        \n",
    "        #返回前5个最相关的电影\n",
    "        for index in list(similarity_index)[:5]:\n",
    "            print('sim: %f,title: %s' %(similarity[index],self.filmTitles[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
