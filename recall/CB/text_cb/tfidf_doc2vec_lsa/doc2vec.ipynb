{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from process.ipynb\n"
     ]
    }
   ],
   "source": [
    "import Ipynb_importer\n",
    "import process\n",
    "from collections import namedtuple\n",
    "import  jieba\n",
    "import codecs\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2vec():\n",
    "    def __init__(self, stopWordsPath, fileTitle, fileIntro):\n",
    "        self.fileIntro=fileIntro\n",
    "        initData = process.Init()\n",
    "        self.stopWords = initData.loadStopWords(stopWordsPath)\n",
    "        self.filmTitles, self.filmDocs = initData.readData(fileTitle, fileIntro)\n",
    "        \n",
    "    def segment(self,text):\n",
    "        \"\"\"分词\"\"\"\n",
    "        words=[word for word in jieba.cut(text) if word not in self.stopWords]\n",
    "        return words\n",
    "    \n",
    "    def textProcess(self):\n",
    "        \"\"\"将每行电影简介文本预处理成doc2vec需要的数据格式\"\"\"\n",
    "        # 将每行文本存于namedtuple中，便于处理\n",
    "        filmText_namedTuple=namedtuple('text','words tags')\n",
    "        count=0\n",
    "        self.filmTextList=[]\n",
    "        with codecs.open(self.fileIntro,'r','utf-8') as file:\n",
    "            for line in file:\n",
    "                # 注意fileText_namedTuple后面一项的id要与self.filmTitles中的id一致，不然在最后的结果显示会出错\n",
    "                count += 1\n",
    "                self.filmTextList.append(filmText_namedTuple(self.segment(line),[count]))\n",
    "                \n",
    "    def trainDoc2vec(self):\n",
    "        # 预处理\n",
    "        self.textProcess()\n",
    "        # 获取cpu核心数\n",
    "        cpuCores=multiprocessing.cpu_count()\n",
    "        # 隐层维数\n",
    "        vec_size=500\n",
    "        \"\"\"\n",
    "        doc2vec参数说明：\n",
    "            dm 定义了训练的算法。默认是dm=1,使用 ‘distributed memory’ (PV-DM)，否则 distributed bag of words (PV-DBOW)。\n",
    "            size 是特征向量的纬度。\n",
    "            window 是要预测的词和文档中用来预测的上下文词之间的最大距离。\n",
    "            alpha 是初始化的学习速率，会随着训练过程线性下降。\n",
    "            seed 是随机数生成器。.需要注意的是，对于一个完全明确的重复运行（fully deterministically-reproducible run），你必须同时限制模型单线程工作以消除操作系统线程调度中的有序抖动。（在python3中，解释器启动的再现要求使用PYTHONHASHSEED环境变量来控制散列随机化）\n",
    "            min_count 忽略总频数小于此的所有的词。\n",
    "            max_vocab_size 在词汇累积的时候限制内存。如果有很多独特的词多于此，则将频率低的删去。每一千万词类大概需要1G的内存，设为None以不限制（默认）。\n",
    "            sample 高频词被随机地降低采样的阈值。默认为0（不降低采样），较为常用的事1e-5。\n",
    "            workers 使用多少现成来训练模型（越快的训练需要越多核的机器）。\n",
    "            iter 语料库的迭代次数。从Word2Vec中继承得到的默认是5，但在已经发布的‘Paragraph Vector’中，设为10或者20是很正常的。\n",
    "            hs 如果为1 (默认)，分层采样将被用于模型训练（否则设为0）。\n",
    "            negative 如果 > 0，将使用负采样，它的值决定干扰词的个数（通常为5-20）。\n",
    "            dm_mean 如果为0（默认），使用上下文词向量的和；如果为1，使用均值。（仅在dm被用在非拼接模型时使用）\n",
    "            dm_concat 如果为1，使用上下文词向量的拼接，默认是0。注意，拼接的结果是一个更大的模型，输入的大小不再是一个词向量（采样或算术结合），而是标签和上下文中所有词结合在一起的大小。\n",
    "            dm_tag_count 每个文件期望的文本标签数，在使用dm_concat模式时默认为1。\n",
    "            dbow_words 如果设为1，训练word-vectors (in skip-gram fashion) 的同时训练 DBOW doc-vector。默认是0 (仅训练doc-vectors时更快)。\n",
    "            trim_rule 词汇表修建规则，用来指定某个词是否要被留下来。被删去或者作默认处理 (如果词的频数< min_count则删去)。可以设为None (将使用min_count)，或者是随时可调参 (word, count, min_count) 并返回util.RULE_DISCARD,util.RULE_KEEP ,util.RULE_DEFAULT之一。注意：这个规则只是在build_vocab()中用来修剪词汇表，而且没被保存。\n",
    "        \"\"\"\n",
    "        doc2vec_model=Doc2Vec(dm=1,dm_concat=0,size=vec_size,window=10,negative=0,hs=0,min_count=1,workers=cpuCores)\n",
    "        # 从一系列句子中建立词汇表\n",
    "        doc2vec_model.build_vocab(self.filmTextList)\n",
    "        \n",
    "        # 训练，训练步数为20，学习速率为0.99\n",
    "        epoch=20\n",
    "        for epo in range(epoch):\n",
    "            try:\n",
    "                print('epoch: %d' % epo)\n",
    "                 #从一系列句子更新模型的神经权重\n",
    "                doc2vec_model.train(self.filmTextList,total_examples=len(self.filmTextList),epochs=doc2vec_model.iter)\n",
    "                #学习速率\n",
    "                doc2vec_model.alpha*=0.99\n",
    "                doc2vec_model.min_alpha=doc2vec_model.alpha\n",
    "            except(KeyboardInterrupt,SystemExit):\n",
    "                break\n",
    "        return doc2vec_model\n",
    "\n",
    "    def doc2VecSearch(self,query):\n",
    "        \"\"\"预测\"\"\"\n",
    "        doc_model=self.trainDoc2vec()\n",
    "        #模型的random设置一个固定值，以返回确定结果\n",
    "        doc_model.random=np.random.RandomState(1)\n",
    "        #infer_vec将查询query转换为词向量\n",
    "        query_vec=doc_model.infer_vector(self.segment(query))\n",
    "        #最相似的文档\n",
    "        doc_sim=doc_model.docvecs.most_similar([query_vec],topn=5)\n",
    "        for sim in doc_sim:\n",
    "            print('sim: %f,title: %s' % (sim[1], self.filmTitles[sim[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for a in range(5):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
