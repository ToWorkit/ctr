{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "训练GBDT_LR\n",
    "'''\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.linear_model import LogisticRegressionCV as LRCV\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "# 将模型整体实例化输出\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_file = \"F:/db/tmp/data/gbdt_test/output_train.txt\"\n",
    "output_feature_num_file = \"F:/db/tmp/data/gbdt_test/feature_num.txt\"\n",
    "output_model_file = \"F:/db/tmp/data/gbdt_test/gbdt.model\"\n",
    "mix_tree_model_file = \"F:/db/tmp/data/gbdt_test/gbdt_tree.model\"\n",
    "mix_lr_model_file = \"F:/db/tmp/data/gbdt_test/gbdt_lr.model\"\n",
    "# 参数化模型\n",
    "mix_lr_model_coef_file = \"F:/db/tmp/data/gbdt_test/gbdt_lr_coef.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到训练数据\n",
    "def get_train_data(train_file, feature_num_file):\n",
    "    '''\n",
    "    :param train_file:\n",
    "    :param feature_num_file:\n",
    "    :return:\n",
    "    '''\n",
    "    # 获取总的特征数目\n",
    "    # total_feature_num = GF.get_feature_num(feature_num_file)\n",
    "    total_feature_num = 103\n",
    "    \n",
    "    # label\n",
    "    train_label = np.genfromtxt(train_file, dtype = np.int32, delimiter = \",\", usecols = -1)\n",
    "    \n",
    "    # feature\n",
    "    feature_list = range(int(total_feature_num))\n",
    "    train_feature = np.genfromtxt(train_file, dtype = np.int32, delimiter = \",\", usecols = feature_list)\n",
    "    return train_feature, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbdt核心代码\n",
    "def train_tree_model_core(train_mat, tree_depth, tree_num, learning_rate):\n",
    "    '''\n",
    "    :param train_mat:\n",
    "        data AND label\n",
    "    :param tree_depth:\n",
    "        深度\n",
    "    :param tree_num:\n",
    "        树的个数\n",
    "    :param learning_rate:\n",
    "        步长\n",
    "    :return:\n",
    "        Booster结构的数据\n",
    "    ''' \n",
    "    \n",
    "    # 优化目标函数: 回归问题的线性优化 \"objective\": \"reg:linear\",\n",
    "    # 不输出的一些信息 \"silent\": 1\n",
    "    para_dict = {\"max_path\": tree_depth, \"eta\": learning_rate, \"objective\": \"reg:squarederror\", \"silent\": 1}\n",
    "    bst = xgb.train(para_dict, train_mat, tree_num)\n",
    "    \n",
    "    # 利用交叉验证（5折交叉验证）查看一些训练指标\n",
    "    # 每一棵树的 auc\n",
    "    print(xgb.cv(para_dict, train_mat, tree_num, nfold = 5, metrics = {\"auc\"}))\n",
    "    \n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择最优参数的参数列表\n",
    "def choose_parameter():\n",
    "    '''\n",
    "    :return:\n",
    "        list\n",
    "            [(tree_depth, tree_num, step_size) ...]\n",
    "    '''\n",
    "    result_list = []\n",
    "    tree_depth_list = [4, 5, 6]\n",
    "    tree_num_list = [10, 50, 100]\n",
    "    learning_rate_list = [0.3, 0.5, 0.7]\n",
    "    for ele_tree_depth in tree_depth_list:\n",
    "        for ele_tree_num in tree_num_list:\n",
    "            for ele_learning_rate in learning_rate_list:\n",
    "                result_list.append((ele_tree_depth, ele_tree_num, ele_learning_rate))\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为gbdt选取最优参数\n",
    "def grid_search(train_mat):\n",
    "    '''\n",
    "    :param train_mat:\n",
    "         data AND label\n",
    "    :return:\n",
    "    '''\n",
    "    # 构建参数列表\n",
    "    para_list = choose_parameter()\n",
    "\n",
    "    for ele in para_list:\n",
    "        (tree_depth, tree_num, learning_rate) = ele\n",
    "        para_dict = {\"max_path\": tree_depth, \"eta\": learning_rate, \"objective\": \"reg:squarederror\", \"silent\": 1}\n",
    "        res = xgb.cv(para_dict, train_mat, tree_num, nfold = 5, metrics = {\"auc\"})\n",
    "        print(res)\n",
    "        auc_score = res.loc[tree_num - 1, [\"test-auc-mean\"]].values[0]\n",
    "        \n",
    "        # GBDT\n",
    "        # 输出每一组参数以及auc的得分\n",
    "        # tree_depth: 6, tree_num: 50, learning_tare: 0.3, auc: 0.923335\n",
    "        # tree_depth: 5, tree_num: 50, learning_tare: 0.3, auc: 0.923335\n",
    "        # tree_depth: 4, tree_num: 50, learning_tare: 0.3, auc: 0.923335\n",
    "        print(\"tree_depth: %s, tree_num: %s, learning_tare: %s, auc: %f\" \\\n",
    "              % (tree_depth, tree_num, learning_rate, auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练gbdt + lr的混合模型\n",
    "# gbdt和lr需要分开单独训练\n",
    "def train_tree_and_lr_model(train_file, feature_num_file, mix_tree_model, mix_lr_model_file):\n",
    "    '''\n",
    "    :param tain_file:\n",
    "        训练数据集\n",
    "    :param feature_num_file:\n",
    "        记录特征维度的文件\n",
    "    :param mix_tree_model:\n",
    "        混合模型 树模型部分的文件\n",
    "    :param mix_lr_model_file:\n",
    "        混合模型 lr部分的文件\n",
    "    '''\n",
    "    train_feature, train_label = get_train_data(train_file, feature_num_file)\n",
    "    # 将数据结构转换为gbdt需要的数据结构\n",
    "    train_mat = xgb.DMatrix(train_feature, train_label)\n",
    "    # 选取的最优参数\n",
    "    # tree_num, tree_depth, learning_rate = 50, 4, 0.3\n",
    "    tree_num = 10\n",
    "    tree_depth = 4\n",
    "    learning_rate = 0.3\n",
    "    '''GBDT'''\n",
    "    # 树模型\n",
    "    bst = train_tree_model_core(train_mat, tree_depth, tree_num, learning_rate)\n",
    "    # 存储树模型\n",
    "    bst.save_model(mix_tree_model)\n",
    "\n",
    "    # lr所需的特征是由树经过编码得到的(只需要将叶子节点编程离散化的特征)\n",
    "    # 用树模型预测样本（看样本最终落在哪个节点上[在 1， 不在 0]）\n",
    "    tree_leaf = bst.predict(train_mat, pred_leaf = True)\n",
    "    \n",
    "    # print(len(tree_leaf), \"30162_tree_leaf\")\n",
    "    # 10棵树，表示每棵树最终样本输出的结果是落到了哪个叶子节点上\n",
    "    # 特征: 样本数 => 1: 100\n",
    "    # 叶子节点: 2 ** tree_depth\n",
    "    # 非叶子节点: 叶子节点 - 1\n",
    "    print(tree_leaf[0])\n",
    "    # print(tree_leaf)\n",
    "    # print(np.max(tree_leaf))\n",
    "    \n",
    "    # 将样本落在哪个节点上的数据进行加工，最终获取训练lr所需要的特征\n",
    "    # 需要叶子节点最终落在了哪个叶子节点[0, 0, 0, 1, 0, 0]\n",
    "    # 特征转换 -> 叶子节点最终落在了哪个叶子节点上\n",
    "    num_leaf_max = np.max(tree_leaf)\n",
    "    total_feature_list = np.zeros([len(tree_leaf),len(tree_leaf[0]) * num_leaf_max], dtype=np.int64)\n",
    "    for i in range(0, len(tree_leaf)):\n",
    "        temp = np.arange(len(tree_leaf[0])) * num_leaf_max - 1 + np.array(tree_leaf[i])\n",
    "        total_feature_list[i][temp] += 1\n",
    "\n",
    "        \n",
    "    '''LR'''\n",
    "    # AUC\n",
    "    # 训练模型\n",
    "    # 参数 [正则化参数], tol 迭代停止条件, max_iter 最大迭代次数, cv 交叉验证(将训练数据分为5份，每次拿20%为测试，80%为训练，一共进行5次), sol 优化方法(使用拟牛顿法, 默认)[希望所有的样本都可以参与到训练当中]\n",
    "    # lr_cf = LRCV(Cs = [1, 10, 100], penalty = \"l2\", tol = 0.0001, max_iter = 500, cv = 5, scoring = \"roc_auc\").fit(total_feature_list, train_label)\n",
    "    lr_cf = LRCV(Cs = [1], \n",
    "                 penalty = \"l2\", \n",
    "                 dual = False, \n",
    "                 tol = 0.0001, \n",
    "                 max_iter = 500, \n",
    "                 cv = 5, \n",
    "                 scoring = \"roc_auc\").fit(total_feature_list, train_label)\n",
    "    \n",
    "    # 5行3列的数组\n",
    "    # scores = lr_cf.scores_.values()[0]\n",
    "    scores = list(lr_cf.scores_.values())[0]\n",
    "    # 每一个正则化参数对应的交叉验证的分值\n",
    "    print(\"diff: %s\" %(\",\".join([str(ele) for ele in scores.mean(axis = 0)])))\n",
    "    # 平均auc\n",
    "    '''\n",
    "    diff: 0.89907602844,0.898857761654,0.89868638722\n",
    "    AUC: 0.898873392438\n",
    "    由此可得第一个参数最优\n",
    "    Cs = [1, 10, 100] => Cs = [1]\n",
    "    '''\n",
    "    print(\"AUC: %s (+-%0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "    '''\n",
    "        GBDT\n",
    "            tree_depth: 4, tree_num: 50, learning_tare: 0.3, auc: 0.923335\n",
    "        \n",
    "        \n",
    "        GBDT + LR\n",
    "            diff: 0.936934169273\n",
    "            AUC: 0.936934169273\n",
    "    '''\n",
    "\n",
    "    # 将模型整体实例化输出\n",
    "    joblib.dump(lr_cf, mix_lr_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.897219       0.002147       0.892260      0.001693\n",
      "1        0.905737       0.001149       0.900225      0.003617\n",
      "2        0.911490       0.000854       0.905588      0.003665\n",
      "3        0.915301       0.000697       0.908965      0.003656\n",
      "4        0.917767       0.001026       0.911052      0.003666\n",
      "5        0.920263       0.001220       0.912948      0.003493\n",
      "6        0.922710       0.001413       0.914579      0.003290\n",
      "7        0.924379       0.001134       0.915629      0.003058\n",
      "8        0.926490       0.000949       0.917177      0.003317\n",
      "9        0.928015       0.000914       0.917806      0.003342\n",
      "[57 65 64 55 61 61 63 57 61 61]\n",
      "diff: 0.9282472511996396\n",
      "AUC: 0.9282472511996396 (+-0.00)\n"
     ]
    }
   ],
   "source": [
    "train_tree_and_lr_model(output_train_file, output_feature_num_file, mix_tree_model_file, mix_lr_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
