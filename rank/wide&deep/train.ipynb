{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$ tf_upgrade_v2 --infile test1.x.py --outfile test2.x.py --reportfile report.txt\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__\n",
    "# tf1.x升级2.x\n",
    "\"\"\"\n",
    "$ tf_upgrade_v2 --infile test1.x.py --outfile test2.x.py --reportfile report.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Expect 15 fields but have 0 in record 0\\n这个错误是因为训练文件或者测试文件的末尾有空行，将空行删除掉即可\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import sys\n",
    "import os\n",
    "\n",
    "'''\n",
    "tensorflow.python.framework.errors_impl.InvalidArgumentError: Expect 15 fields but have 0 in record 0\n",
    "这个错误是因为训练文件或者测试文件的末尾有空行，将空行删除掉即可\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"F:/db/tmp/data/lr/train.csv\"\n",
    "test_file = \"F:/db/tmp/data/lr/test.csv\"\n",
    "\n",
    "output_model = \"F:/db/tmp/data/wd_test/model\"\n",
    "output_feature_num_file = \"F:/db/tmp/data/wd_test/feature_num.txt\"\n",
    "# 需要提前创建好\n",
    "output_model_serving = \"F:/db/tmp/data/wd_test/model_serving\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取wide侧和deep侧特征\n",
    "def get_feature_column():\n",
    "    '''\n",
    "        所有的特征\n",
    "            age,workclass,education,education-num,marital-status,occupation,relationship,race,sex,capital-gain,capital-loss,hours-per-week,native-country,label\n",
    "        :return\n",
    "            wide feature column\n",
    "            deep feature column\n",
    "    '''\n",
    "    # 连续型特征\n",
    "    age = tf.feature_column.numeric_column(\"age\")\n",
    "    education_num = tf.feature_column.numeric_column(\"education-num\")\n",
    "    capital_gain = tf.feature_column.numeric_column(\"capital-gain\")\n",
    "    capital_loss = tf.feature_column.numeric_column(\"capital-loss\")\n",
    "    hours_per_week = tf.feature_column.numeric_column(\"hours-per-week\")\n",
    "\n",
    "    # hash\n",
    "    # 离散型特征先进行hash之后放入wide侧然后再进行embedding最后将embedding得到的结果放入deep侧\n",
    "    # 因为特征的取值范围不会超过 512 暂时不使用太多的\n",
    "    work_class = tf.feature_column.categorical_column_with_hash_bucket(\"workclass\", hash_bucket_size = 512)\n",
    "    education = tf.feature_column.categorical_column_with_hash_bucket(\"education\", hash_bucket_size = 512)\n",
    "    marital_status = tf.feature_column.categorical_column_with_hash_bucket(\"marital-status\", hash_bucket_size = 512)\n",
    "    occupation = tf.feature_column.categorical_column_with_hash_bucket(\"occupation\", hash_bucket_size = 512)\n",
    "    relationship = tf.feature_column.categorical_column_with_hash_bucket(\"relationship\", hash_bucket_size = 512)\n",
    "\n",
    "    # 离散化\n",
    "    # 对 年龄，收入，支出 作离散化\n",
    "    # 年龄段分割 boundaries\n",
    "    age_bucket = tf.feature_column.bucketized_column(age, boundaries = [18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "    gain_bucket = tf.feature_column.bucketized_column(capital_gain, boundaries = [0, 1000, 2000, 3000, 10000])\n",
    "    loss_bucket = tf.feature_column.bucketized_column(capital_loss, boundaries = [0, 1000, 2000, 3000, 5000])\n",
    "\n",
    "    # 交叉\n",
    "    # 构建交叉特征\n",
    "    cross_columns = [\n",
    "        # 将年龄和收入交叉\n",
    "        # 年龄分为了9段，收入分为了4段，所以交叉完是36段\n",
    "        tf.feature_column.crossed_column([age_bucket, gain_bucket], hash_bucket_size = 36),\n",
    "        # 收入和支出交叉\n",
    "        tf.feature_column.crossed_column([gain_bucket, loss_bucket], hash_bucket_size = 16),\n",
    "    ]\n",
    "\n",
    "    # wide侧特征包括 hash，离散化，交叉\n",
    "    base_columns = [work_class, education, marital_status, occupation, relationship, age_bucket, gain_bucket, loss_bucket,]\n",
    "    wide_columns = base_columns + cross_columns\n",
    "\n",
    "    # deep侧特征包括 连续 hash值的embedding\n",
    "    deep_columns = [\n",
    "        age,\n",
    "        education_num,\n",
    "        capital_gain,\n",
    "        capital_loss,\n",
    "        hours_per_week,\n",
    "        # 向量的维度选择 9 维，因为 2 ^ 9 是 512，可以涵盖上面的hash\n",
    "        tf.feature_column.embedding_column(work_class, 9),\n",
    "        tf.feature_column.embedding_column(education, 9),\n",
    "        tf.feature_column.embedding_column(marital_status, 9),\n",
    "        tf.feature_column.embedding_column(occupation, 9),\n",
    "        tf.feature_column.embedding_column(relationship, 9),\n",
    "    ]\n",
    "\n",
    "    return wide_columns, deep_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_estimator(wide_column, deep_column, model_folder):\n",
    "    '''\n",
    "    :param wide_column:\n",
    "    :param deep_column:\n",
    "    :param model_folder:\n",
    "    :return:\n",
    "        2\n",
    "            模型的实例\n",
    "            辅助模型导出提供服务的函数\n",
    "    '''\n",
    "    # 使用高阶api\n",
    "    model_es = tf.compat.v1.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir = model_folder,\n",
    "        # wide特征\n",
    "        linear_feature_columns = wide_column,\n",
    "        # 优化器(学习率，l2正则化=正则化参数)\n",
    "        linear_optimizer = tf.compat.v1.train.FtrlOptimizer(0.1, l2_regularization_strength = 1.0),\n",
    "        # deep特征\n",
    "        dnn_feature_columns = deep_column,\n",
    "        # 优化器(学习率，l1正则化=正则化参数, l2正则化=正则化参数)[不同优化器之间的目的都是为了控制参数迭代过程的平稳，每次迭代的幅度不要太大]\n",
    "        dnn_optimizer = tf.compat.v1.train.ProximalAdagradOptimizer(learning_rate = 0.1, l1_regularization_strength = 0.001, l2_regularization_strength = 0.001),\n",
    "        # 隐层的维度(4层)[隐层的节点个数决定参数的整体维度]\n",
    "        dnn_hidden_units = [128, 64, 32, 16],\n",
    "    )\n",
    "\n",
    "    # 辅助函数\n",
    "    # 所有的特征\n",
    "    feature_column = wide_column + deep_column\n",
    "    feature_spec = tf.feature_column.make_parse_example_spec(feature_column)\n",
    "    serving_input_fn = (tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec))\n",
    "\n",
    "    return model_es, serving_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, re_time, shuffle, batch_num, predict):\n",
    "    '''\n",
    "    :param data_file:\n",
    "        输入文件\n",
    "            训练文件或者测试文件\n",
    "    :param re_time:\n",
    "        重复采样的次数\n",
    "    :param shuffle:\n",
    "        是否要打乱数据 Boolean\n",
    "    :param batch_num:\n",
    "        采用随机梯度下降时，多少个样本更新一次参数\n",
    "    :param predict:\n",
    "        训练还是测试 Boolean\n",
    "    :return:\n",
    "        2\n",
    "            训练的feature和label\n",
    "            测试的feature\n",
    "    '''\n",
    "    # print(data_file, \"data_file\")\n",
    "    # sys.exit()\n",
    "\n",
    "    _CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''], [0], [0], [0], [''], ['']]\n",
    "\n",
    "    _CSV_COLUMNS = [\n",
    "        \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "        \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "        \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\",\n",
    "        \"label\"\n",
    "    ]\n",
    "\n",
    "    def parse_csv(value):\n",
    "        columns = tf.io.decode_csv(value, record_defaults = _CSV_COLUMN_DEFAULTS)\n",
    "        # 字典形式，key是上面的特征名称，value是稀疏列表[0,0,0,0,1,0,0 ...]\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop(\"label\")\n",
    "        classes = tf.equal(labels, \">50K\")\n",
    "        return features, classes\n",
    "\n",
    "    # 预测时不需要返回label\n",
    "    def parse_csv_predict(value):\n",
    "        columns = tf.io.decode_csv(value, record_defaults = _CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop(\"label\")\n",
    "        return features\n",
    "\n",
    "    # skip(1) 过滤第一行特征名称以及有 ? 的行，数据清洗\n",
    "    data_set = tf.data.TextLineDataset(data_file).skip(1).filter(lambda line: tf.not_equal(tf.strings.regex_full_match(line, \".*\\?.*\"), True))\n",
    "    # data_set = tf.data.TextLineDataset(data_file).skip(1)\n",
    "\n",
    "    if shuffle:\n",
    "        data_set = data_set.shuffle(buffer_size = 30000)\n",
    "    if predict:\n",
    "        data_set = data_set.map(parse_csv_predict, num_parallel_calls = 5)\n",
    "    else:\n",
    "        data_set = data_set.map(parse_csv, num_parallel_calls = 5)\n",
    "\n",
    "    # 重复采样\n",
    "    data_set = data_set.repeat(re_time)\n",
    "    # 分割测试或者训练\n",
    "    data_set = data_set.batch(batch_num)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wd_model(model_es, train_file, test_file, model_export_folder, serving_input_fn):\n",
    "    '''\n",
    "    :param model_es:\n",
    "        模型的对象\n",
    "    :param train_file:\n",
    "    :param test_file:\n",
    "    :param model_export_folder:\n",
    "        模型导出的文件夹\n",
    "    :param serving_input_fn:\n",
    "        辅助模型导出提供服务的函数\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    '''重复采样'''\n",
    "    # 利用重复采样来扩充训练集(弥补数据集小)\n",
    "    total_run = 6\n",
    "    for index in range(total_run):\n",
    "        # 模型训练\n",
    "        # 每一次重复采样10倍\n",
    "        model_es.train(input_fn = lambda: input_fn(train_file, 10, True, 100, False))\n",
    "        # 模型评估\n",
    "        print(model_es.evaluate(input_fn = lambda: input_fn(test_file, 1, False, 100, False)))\n",
    "\n",
    "    '''不重复采样'''\n",
    "    # 模型训练\n",
    "    # model_es.train(input_fn = lambda: input_fn(train_file, 20, True, 100, False))\n",
    "    # # 模型评估\n",
    "    # # {'accuracy': 0.8512616, 'accuracy_baseline': 0.7543161, 'auc': 0.9083664, 'auc_precision_recall': 0.77997875, 'average_loss': 0.31840968, 'label/mean': 0.24568394, 'loss': 31.756622, 'precision': 0.7321883, 'prediction/mean': 0.25312397, 'recall': 0.62216216, 'global_step': 6033}\n",
    "    # print(model_es.evaluate(input_fn = lambda: input_fn(test_file, 1, False, 100, False)))\n",
    "\n",
    "\n",
    "    # 模型导出（为了给tf_server提供服务的）\n",
    "    model_es.export_saved_model(model_export_folder, serving_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到测试label\n",
    "def get_test_label(test_file):\n",
    "    if not os.path.exists(test_file):\n",
    "        return []\n",
    "    fp = open(test_file, \"r\", encoding = \"utf-8\")\n",
    "    line_num = 0\n",
    "    test_label_list = []\n",
    "    for line in fp:\n",
    "        if (line):\n",
    "            if line_num == 0:\n",
    "                line_num += 1\n",
    "                continue\n",
    "            if \"?\" in line.strip():\n",
    "                continue\n",
    "            item = line.strip().split(\",\")\n",
    "            # 取最后一行\n",
    "            label = item[-1].strip()\n",
    "            if label == \">50K\":\n",
    "                test_label_list.append(1)\n",
    "            elif label == \"<=50K\":\n",
    "                test_label_list.append(0)\n",
    "            else:\n",
    "                print(test_file, \"error\", line)\n",
    "\n",
    "        # print(test_label_list)\n",
    "    fp.close()\n",
    "    return test_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def get_accuracy(predict_list, test_label):\n",
    "    # 临界值 大于 正样本，小于 负样本\n",
    "    score_thr = 0.8\n",
    "    # 预测对的\n",
    "    right_num = 0\n",
    "    predict_label_list = []\n",
    "    for index in range(len(predict_list)):\n",
    "        predict_score = predict_list[index]\n",
    "        if predict_score >= score_thr:\n",
    "            predict_label = 1\n",
    "        else:\n",
    "            predict_label = 0\n",
    "        predict_label_list.append(predict_label)\n",
    "        if predict_label == test_label[index]:\n",
    "            # 预测对的\n",
    "            right_num += 1\n",
    "    '''\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] [ 0.  0.  1.  1.  0.  1.  0.  0.  1.  0.]\n",
    "    accuary: 0.80737\n",
    "    [0, 0, 0, 1, 0, 1, 0, 0, 1, 1] [ 0.  0.  1.  1.  0.  1.  0.  0.  1.  0.]\n",
    "    accuary: 0.83778\n",
    "    '''\n",
    "    # predict_label_list 预测的结果(可以将测试集去label来模拟真实需要预测的数据)\n",
    "    # test_label 实际的结果\n",
    "    print(predict_label_list[:10], test_label[:10])\n",
    "    total_num = len(predict_list)\n",
    "    accuracy_score = right_num / total_num\n",
    "    print(\"accuracy: %.5f\" %(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc\n",
    "def get_auc(predict_list, test_label):\n",
    "    '''\n",
    "    :param predict_list:\n",
    "        模型预测label\n",
    "    :param test_label:\n",
    "        测试label\n",
    "    # pos 正样本\n",
    "    auc = (sum(pos_index) - pos_num(pos_num + 1) / 2) / pos_num * neg_num\n",
    "    '''\n",
    "    total_list = []\n",
    "    for index in range(len(predict_list)):\n",
    "        predict_score = predict_list[index]\n",
    "        label = test_label[index]\n",
    "        total_list.append((label, predict_score))\n",
    "    # 排序\n",
    "    sorted_total_list = sorted(total_list, key = lambda ele: ele[1])\n",
    "\n",
    "    # 负样本\n",
    "    neg_num = 0\n",
    "    # 正样本\n",
    "    pos_num = 0\n",
    "    count = 1\n",
    "    total_pos_index = 0\n",
    "    for zuhe in sorted_total_list:\n",
    "        label, predict_score = zuhe\n",
    "        if label == 0:\n",
    "            neg_num += 1\n",
    "        else:\n",
    "            pos_num += 1\n",
    "            # 所有正样本的index + 所处的位置\n",
    "            total_pos_index += count\n",
    "        count += 1\n",
    "    auc_score = (total_pos_index - pos_num * (pos_num + 1) / 2) / (pos_num * neg_num)\n",
    "    print(\"auc: %5f\" %(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测评\n",
    "def test_model_performance(model_es, test_file):\n",
    "    '''\n",
    "    :param model_es:\n",
    "    :param test_file:\n",
    "    :return:\n",
    "    '''\n",
    "    test_label = get_test_label(test_file)\n",
    "    # 预测\n",
    "    result = model_es.predict(input_fn = lambda : input_fn(test_file, 1, False, 100, True))\n",
    "    # 提取每一个样本的预测结果\n",
    "    predict_list = []\n",
    "    for one_res in result:\n",
    "        if \"probabilities\" in one_res:\n",
    "            predict_list.append(one_res[\"probabilities\"][1])\n",
    "    '''\n",
    "    auc: 0.908438\n",
    "    预测的                           实际的\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "    accuary: 0.81195\n",
    "    '''\n",
    "    get_auc(predict_list, test_label)\n",
    "    get_accuracy(predict_list, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main(train_file, test_file, model_folder, model_export_folder):\n",
    "    '''\n",
    "    :param train_file:\n",
    "    :param test_file:\n",
    "    :param model_folder:\n",
    "        原始模型导出到的文件夹\n",
    "    :param model_export_folder:\n",
    "        提供给 TF Server 作 model server服务调用的导出文件夹\n",
    "        含有通信所需要的PB\n",
    "    :return:\n",
    "    '''\n",
    "    # wide侧和deep侧特征\n",
    "    wide_column, deep_column = get_feature_column()\n",
    "    # 构造模型\n",
    "    model_es, serving_input_fn = build_model_estimator(wide_column, deep_column, model_folder)\n",
    "    # 训练模型\n",
    "    '''\n",
    "    model\n",
    "        生成的checkpoint文件中記載着最新的model，下次训练时模型会将最新model中的w和b加载到内存中继续训练\n",
    "            比如123，那么下次就会从456开始\n",
    "            \n",
    "    model_serving(提供服务的)\n",
    "        生成的是秒级的时间戳文件夹，tf server可以直接加载并且提供服务\n",
    "        *.pb 文件是用来通信的\n",
    "        variables\n",
    "            模型训练得到的参数\n",
    "    '''\n",
    "    train_wd_model(model_es, train_file, test_file, model_export_folder, serving_input_fn)\n",
    "\n",
    "    # 利用测试数据集评估模型\n",
    "    test_model_performance(model_es, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0107 17:27:00.371400  9132 deprecation.py:506] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "W0107 17:27:00.381400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0107 17:27:01.425400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py:518: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "W0107 17:27:01.906400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\canned\\linear.py:308: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0107 17:27:03.640400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "W0107 17:27:25.667400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\training\\saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "W0107 17:27:27.507400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0107 17:27:27.818400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0107 17:27:27.841400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8505312, 'accuracy_baseline': 0.7543161, 'auc': 0.9038848, 'auc_precision_recall': 0.7670498, 'average_loss': 0.32521477, 'label/mean': 0.24568394, 'loss': 32.435326, 'precision': 0.7325843, 'prediction/mean': 0.25261292, 'recall': 0.61675674, 'global_step': 21119}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 17:27:58.207400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0107 17:27:58.232400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8498672, 'accuracy_baseline': 0.7543161, 'auc': 0.9039841, 'auc_precision_recall': 0.76708174, 'average_loss': 0.325002, 'label/mean': 0.24568394, 'loss': 32.41411, 'precision': 0.74120015, 'prediction/mean': 0.24450599, 'recall': 0.59756756, 'global_step': 24136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 17:28:27.560400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0107 17:28:27.584400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8504648, 'accuracy_baseline': 0.7543161, 'auc': 0.90391123, 'auc_precision_recall': 0.7670443, 'average_loss': 0.32500234, 'label/mean': 0.24568394, 'loss': 32.41414, 'precision': 0.7389439, 'prediction/mean': 0.2462818, 'recall': 0.60513514, 'global_step': 27153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 17:28:56.083400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0107 17:28:56.107400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.85, 'accuracy_baseline': 0.7543161, 'auc': 0.9039383, 'auc_precision_recall': 0.7670508, 'average_loss': 0.325013, 'label/mean': 0.24568394, 'loss': 32.415203, 'precision': 0.7400866, 'prediction/mean': 0.24494864, 'recall': 0.6002703, 'global_step': 30170}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 17:29:25.032400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0107 17:29:25.059400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.85, 'accuracy_baseline': 0.7543161, 'auc': 0.90394866, 'auc_precision_recall': 0.76709276, 'average_loss': 0.32500777, 'label/mean': 0.24568394, 'loss': 32.414684, 'precision': 0.7396076, 'prediction/mean': 0.24533652, 'recall': 0.6010811, 'global_step': 33187}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 17:29:54.433400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "W0107 17:29:54.454400  9132 metrics_impl.py:803] Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8502656, 'accuracy_baseline': 0.7543161, 'auc': 0.90393347, 'auc_precision_recall': 0.7671133, 'average_loss': 0.3250494, 'label/mean': 0.24568394, 'loss': 32.418835, 'precision': 0.73495936, 'prediction/mean': 0.24852645, 'recall': 0.6108108, 'global_step': 36204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0107 17:29:57.448400  9132 deprecation.py:323] From c:\\users\\hedy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\signature_def_utils_impl.py:145: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.903942\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
      "accuracy: 0.81341\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_main(train_file, test_file, output_model, output_model_serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
